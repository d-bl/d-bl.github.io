name: Check links of  d-bl repos

on:
  workflow_dispatch:

jobs:
  check-links:
    runs-on: ubuntu-latest
    continue-on-error: true # in case some artifacts not available
    strategy:
      max-parallel: 1 # prevent cancel of artifact downloads
      matrix:
        repo:
          - d-bl.github.io
          - GroundForge
          - MAE-gf
          - GroundForge-help
          - tesselace-to-gf
          - inkscape-bobbinlace
          - polar-grids
          - gw-lace-to-gf # a lot of requests on https://archive.org/details/laceguideformak00whit/page/
    steps:
      - name: Get latest successful run ID for ${{ matrix.repo }}
        id: get_run_id
        run: |
          run_id=$(gh run list --repo d-bl/${{ matrix.repo }} --workflow pages-build-deployment --status success --limit 1 --json databaseId -q '.[0].databaseId')
          echo "run_id=$run_id" >> $GITHUB_OUTPUT
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Download github-pages artifact of ${{ matrix.repo }}
        uses: actions/download-artifact@v5
        with:
          repository: d-bl/${{ matrix.repo }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ steps.get_run_id.outputs.run_id }}
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Untar downloaded artifact of ${{ matrix.repo }}
        run: |
          tar -xf artifact.tar

      - name: List extracted files of ${{ matrix.repo }}
        run: |
          pwd
          ls -R ../..

      - name: Get extract-links.py
        run: |
          wget https://raw.githubusercontent.com/d-bl/d-bl.github.io/refs/heads/master/.github/workflows/extract-links.py
        continue-on-error: false

      - name: Collect URLs from extracted HTML pages
        run: |
          python extract-links.py ${{ matrix.repo }} > urls-with-anchors.txt
        shell: bash

      - name: Show collected urls
        run: |
          cat urls-with-anchors.txt

      - name: Show problematic URLs
        run: |
          cat urls-with-anchors.txt | cut -d ' ' -f1 | sort | uniq > collected-urls.txt
          xargs -I {} bash -c 'curl -s -o /dev/null -w "%{http_code}" "{}" ; echo " " {} ; sleep 1' < collected-urls.txt | egrep -v '(301  |302  |200  )'
