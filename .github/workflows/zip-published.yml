name: Analyze Latest Page Build Artifacts

on:
  workflow_dispatch:

jobs:
  download-assets:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repo:
          - d-bl.github.io
          - GroundForge
#          - GroundForge-help
#          - MAE-gf
#          - tesselace-to-gf
#          - gw-lace-to-gf
#          - inkscape-bobbinlace
#          - polar-grids
    steps:

      - name: Get latest successful run ID for ${{ matrix.repo }}
        id: get_run_id
        run: |
          run_id=$(gh run list --repo d-bl/${{ matrix.repo }} --workflow pages-build-deployment --status success --limit 1 --json databaseId -q '.[0].databaseId')
          echo "run_id=$run_id" >> $GITHUB_OUTPUT
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Download github-pages artifact of ${{ matrix.repo }}
        uses: actions/download-artifact@v5
        with:
          repository: d-bl/${{ matrix.repo }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ steps.get_run_id.outputs.run_id }}
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Untar downloaded artifact of ${{ matrix.repo }}
        run: tar -xf artifact.tar

      - name: List extracted files of ${{ matrix.repo }}
        run: |
          pwd
          ls -R ..

      - name: Collect URLs from extracted HTML pages
        run: |
          import os
          from html.parser import HTMLParser
          
          class URLExtractor(HTMLParser):
            def __init__(self):
              super().__init__()
              self.urls = set()
              self.attrs = {'href', 'src', 'data-src', 'data-href', 'action', 'poster'}
            
            def handle_starttag(self, tag, attrs):
              for attr, value in attrs:
                if attr.lower() in self.attrs and value:
                  self.urls.add(value.split('?')[0])
            
          urls = set()
          for root, _, files in os.walk('.'):
            for file in files:
              if file.endswith('.html'):
                with open(os.path.join(root, file), encoding='utf-8') as f:
                  parser = URLExtractor()
                  parser.feed(f.read())
                  urls.update(parser.urls)
          
          with open('collected-urls.txt', 'w', encoding='utf-8') as out:
            for url in sorted(urls):
              out.write(url + '\n')

        shell: python

      - name: Fix collected urls
        run: |
          sed 's!html:!html === !' collected-urls.txt | \
            sed 's! /! https://d-bl.github.io/!' | \
            sed -E 's!(=== )(/|:/)!=== https://d-bl.github.io/!' | \
            sort -u > fixed-urls.txt
          cat fixed-urls.txt          
